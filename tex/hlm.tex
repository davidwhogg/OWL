\documentclass[12pt, letterpaper, preprint]{aastex}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\transpose}[1]{{#1}^{\!\mathsf T}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\E}[1]{\mathrm{E}[{#1}]}
\newcommand{\Var}[1]{\mathrm{Var}[{#1}]}
\newcommand{\Covar}[1]{\mathrm{Covar}[{#1}]}

\begin{document}

\title{
  Optimized ``soft'' aperture photometry for time-domain astronomy
}
\author{
  David W. Hogg,
  Dan Foreman-Mackey,
  Jonathan Goodman,
  Fengji Hou,
  others
}

\begin{abstract}
Precise stellar photometry campaigns by \project{Kepler}, \project{Spitzer}, and \project{PanSTARRS},
  among many others,
  create precise lightcurves by fixing the star in detector coordinates,
  and estimating brightness with a fixed linear combination of background-subtracted pixel values.
Under any complete set of assumptions about the noise in the detector,
  there is a particular choice of pixel weights for use in the brightness estimate
  that maximizes signal-to-noise.
When precision requirements are extremely high,
  the noise model must necessarily include pointing and point-spread-function variations,
  which introduce highly correlated noise at the pixel level.
Here we generalize previous results on optimal photometric estimators
  to include arbitrarily correlated pixels.
Amusingly, the approach we advocate does not require any advance knowledge of any noise source;
  once there are enough epochs observed,
  the pixel-level data are sufficient to build an empirical data-driven correlated noise model.
We demonstrate enormous improvements to the \project{Kepler} photometry of isolated stars.
The best soft apertures produce photometric measurements so uniform that
  they outperform or obviate much more heuristic (and much more scientifically damaging)
  ``detrending'' sschemes in many real cases.
\end{abstract}

\keywords{
  instrumentation: photometers
  ---
  methods: data analysis
  ---
  methods: statistical
  ---
  techniques: photometric
}

\section{Rules of the photometry game}

Precise photometry is the bread-and-butter of the exoplanet community.
The unprecedented precision of the \project{Kepler} photometry has led to the discovery of
dozens of Earth-radius planets (HOGG CITES).
Precise photometry is, no doubt, going to be of great importance in many other areas,
  but we don't care that much about any of them (besides exoplanets) at present.

Imagine you are trying to do very sensitive relative photometry on a variable source.
The most precise methodologies at present involve fixing the position of the source on the detector
  (cite examples)
  to reduce dependency of the results on the flat-field or flat-fielding errors.
They also often involve defocusing the telescope
  (cite examples),
  to illuminate the pixels more uniformly
  (and hence reduce intra-pixel sensitivity issues),
  avoid saturation,
  and make the observations less atmosphere-dependent.
There are also strategies related to calibration or the choices of comparison stars
  (cite examples).
Once all these choices are made,
  the choice of methodology for actually photometering the source remains.
Most projects choose something akin to aperture photometry
  (cite examples),
  in which the pixels within an aperture are co-added with unit weights,
  and pixels outside the aperture are not used at all.
Here we ask only about this last choice:
What is the best way to measure the brightness of a nearly constant source,
  given a set of precise images of the source taken over time?

We are going to answer this question using optimization, of course;
  we are going to find the ``best'' method.
We will assume that the investigator obeyed the usual rules,
  so he or she fixed the position of the star on the detector,
  defocused the telescope,
  and avoided saturation.
However, we are also going to assume---%
  as is \emph{always the case}---%
  that there are some residual,
  unaccounted-for positional offsets (jitter) and point-spread-function changes.
These offsets and changes could be random or systematic over time,
  but we will assume that they are unknown \foreign{a priori}.
That is, the telescope house-keeping data are not good enough to track them at the level we care about.
We are trying to measure \emph{extremely} precise photometry,
  at the level of $10^{-4}$ or $10^{-5}$ or better.

The conditions we have assumed might seem strange,
  but they are generic for the most precise photometry systems currently operating.
\project{Kepler}, for instance, has all of these problems,
  as does [insert projects here, like Wright's].

% in what follows the object of photometry is a ``star'' not a ``source''.

It is possible to think of this problem as the problem of finding the best strategy in a single-player game.
\emph{The Rules} of this game are as follows:
\begin{itemize}
\item
  There is copious multi-epoch single-band imaging,
  with many epochs (many images),
  and a known position in that imaging of a star of great interest (the ``object'' star).
  There is also perhaps some ``pixel mask'' indicating what pixels are permitted for use
  in the vicinity of that star.
\item
  There may or may not be a list of ``comparison'' stars
  that are used to calibrate or ratio the photometry of the object star.
  If there aren't, then it can be assumed that the imaging is very well calibrated.
\item
  It is expected that the star is fairly stationary in device coordinates (pixel location).
  However, in each image it has been offset slightly by some amount from its fiducial position.
  There are no reliable meta-data to help you understand those small offsets.
  Any comparison stars are also subject to small offsets;
  the comparison-star offsets are not necessarily the same as the object-star offsets,
  because there is camera rotation, variable optical distortion, proper motion, and parallax.
\item
  It is expected that the point-spread function (PSF) is fairly constant.
  However, again, in each image it has a small difference from the fiducial point-spread function.
  There are no reliable meta-data to help with this,
  and the comparison star PSFs will also vary but not in precisely the same way,
  though their variations might be non-trivially correlated.
\item
  There are no reliable meta-data about read noise or gains,
  so there is no precise noise estimate for any pixel at any epoch.
\item
  When you photometer a star,
  the only operation you are permitted is linear weighted sums of pixels in the images.
  Furthermore, any weighted sums must be precisely identical from epoch to epoch.
  That is, the ``aperture''
  (which in fact may be some complicated set of pixel weights)
  is required to be identical from image to image.
\end{itemize}
These rules are generic;
  this is a common situation for precise, multi-epoch photometry.

The only rule to which we object is the last one:
In principle it should be better to do focal-plane modeling
  and perform photometry as a maximum-likelihood value
  or some statistics of a posterior PDF
  based on a parameterized likelihood function (for example, \citealt{hoggwhitepaper}).
However, there are many applications in which
  the physical information one would like for building such a model is not available.
We would parameterize and fit nonetheless,
  but most astronomical projects don't take this approach at present.
There are also situations in which the output of full likelihood fitting for the data
  shows correlations between the final photometry and nuisance parameters (such as PSF parameters).
In these cases, the investigator is left wondering if the residual photometric variance
  has contributions from nuisance-parameter variance or fit residuals or model mismatch.

There have been some previous attempts to optimize photometric estimators subject to \emph{The Rules},
  in generic (but limiting) situations.
When the pointing and point-spread function are both known precisely,
  the pixel noise variance is known for every pixel,
  and the pixel noise is independent (uncorrelated) pixel-to-pixel,
  there are simple analytic results (\cite{naylor}).
These results reduce to the psf-as-matched-filter standard practice
  in the further simplification of sky-dominated noise.
There are no previous results (to our knowledge) that take account of
  the highly covariant noise created by variable pointing or optics.
This is the first primary novelty of this work.
The second primary novelty is the realization that the noise---%
  even the covariant part created by optics and pointing---%
  can be determined empirically from the data themselves,
  with no prior knowledge of the characteristics of the detector,
  telescope, or atmosphere.

\section{Well-calibrated data}

In the (possibly fantastical) situation in which the imaging data are all properly calibrated,
  we have no need for comparison stars.
We are going do the photometry using nothing but the pixels near the object star.

Before we say what ``properly calibrated'' means,
  it is necessary to specify what we mean by an ``image''.
Ordinarily it is our view that
  an image is a measurement of the smoothed intensity field
  or smoothed photon phase-space density.
It ought to have units of energy per time per area per solid angle
  or photons per time per area per solid angle.
The problem with this view of an image
  is that if we do aperture photometry---%
  that is, if we make linear weighted sums of image pixels---%
  to get a total stellar flux,
  the weights in the weighted sum must have units of \emph{solid angle}.
Small changes to the optics or atmosphere will change the astrometric mapping of the camera,
  changing the solid angles of the pixels.
Aperture photometry is the wrong thing to do in this formulation of an image;
  or it is an okay thing to do if the weights are permitted to vary with the astrometric distortion.
That would violate \emph{The Rules} above.

So the image we want for these purposes is not a measurement of the intensity field,
  but rather a kind of ``flux map''.
It ought to have units of energy per time per area
  or photons per time per area.
These values can be co-added with dimensionless weights to deliver a stellar flux.
The only difference between an intensity image and a flux map is a factor of
  the angular size of the pixels.
In many imagers pixels are similar in size so these differences aren't apparent.
However, at the levels of precision we care about here
  ($10^{-4}$ or $10^{-5}$ or better)
  these details matter.

What is meant by ``properly calibrated''?
The image (flux map) is properly calibrated when it is can be treated as a measurement of---%
  or is directly proportional to---%
  a smoothed map of energy or photon number per time per area.
It is ``smoothed'' because it is convolved (or correlated) with some finite
  and possibly spatially varying PSF
  (which we don't know).

NEED TO SAY SOMETHING ABOUT ZERO-MEAN RESIDUAL BACKGROUND SUBTRACTION.

Are any imaging data properly calibrated according to these criteria?
We don't know, but the \project{Kepler} Satellite may be.
The standard data products of the \project{Hubble Space Telescope}
  are given in intensity units,
  but the astrometric solution is known to high precision,
  so (with a tiny bit of work)
  these also can be properly calibrated for aperture photometry according to this definition.

We do not endorse the view that images should be presented as flux maps!
We are just sayin' that \emph{if} you need to obey \emph{The Rules},
  then you need to convert your intensity maps into flux maps before playing the game.

\section{Emprirically optimal photometry}

We have at each of $N$ epochs $n$ a set of $D$ pixels in a small patch
  that contains a star.
The set of measured, properly calibrated (see above),
  correctly background-subtracted (see above)
  pixel values at epoch $n$ form a $D$-dimensional column vector $y_n$.
According to \emph{The Rules}, all we are permitted to do at epoch $n$
  is produce a weighted sum
\begin{eqnarray}
q_n &\leftarrow& \transpose{w}\, y_n
\quad ,
\end{eqnarray}
  where $q_n$ is a scalar photometric measurement,
  $w$ is a $D$-dimensional column vector of weights,
  and (because of the transpose)
  the product is effectively an inner (scalar or dot) product.
\emph{The Rules} state that the weight vector $w$ must not depend on $n$ or anything else;
  the weights are the same at every epoch.

We can see $q_n$ as an estimator---a point estimate---of the stellar brightness.
If we knew the expectation value (mean) for the pixel data $y_n$ and also the
  variance of those data around the mean,
  we could predict the expectation and variance of the estimator.
The expectation value would be:
\begin{eqnarray}
\E{q_n} &=& \transpose{w}\, \mu_n
\\
\mu_n &\equiv& \E{y_n}
\\
\Var{q_n} &=& \transpose{w}\, C_n\, w
\\
C_n &\equiv& \Covar{y_n}
\quad ,
\end{eqnarray}
  where $\E{x}$ is the expectation value of $x$,
  $\Var{x}$ is the variance of (a scalar) $x$ around its expectation value,
  and $\Covar{x}$ is the $D\times D$ symmetric, positive definite covariance matrix
  of (a vector) $x$ around its expectation value.
In the simple case of independent noise from pixel-to-pixel,
  $\Covar{y_n}$ becomes diagonal,
  and the variance $\Var{q_n}$ becomes simply the sum of the diagonal variances,
  weighted by the squared weights.
The expected squared signal-to-noise ratio $\Sigma^2$ for $q_n$ is
\begin{eqnarray}
\Sigma^2 &=& \frac{[\transpose{w}\, \mu_n]^2}{\transpose{w}\, C_n\, w}
\quad .
\end{eqnarray}

BAD SYMBOL ABOVE; $n$ OVERLOADED

It is almost always assumed that the pixel-to-pixel noise is independent (diagonal);
  why do we not assume this here?
The answer lies in \emph{The Rules}:
Variations of the pointing and point-spread function (and background),
  which are all unknown,
  correlate the pixels.
That is, they produce a kind of noise that is neither photon nor read noise in the detector.
They produce a jitter noise that leads to covariant variations of the individual pixel values.

We win the game if we find the weight vector $w$ that optimizes signal-to-noise.
It can't optimize the signal-to-noise at epoch $n$;
  it must optimize the mean or typical signal-to-noise across epochs.
That is, we want to find the weight vector $w$ that optimizes
\begin{eqnarray}
\Sigma^2 &=& \frac{[\transpose{w}\, \mu]^2}{\transpose{w}\, C\, w}
\quad ,
\end{eqnarray}
where now $\mu$ is the expectation value of the data vectors $y_n$ but now over \emph{all} epochs,
  and $C$ is the covariance matrix for the data but again over \emph{all} epochs.
The signal-to-noise-optimal weight vector is simply
\begin{eqnarray}
w &\leftarrow& \inverse{C}\, \mu
\quad ,
\end{eqnarray}
  where the inverse operation is the full matrix inverse.
This optimal-weight result is the natural generalization to correlated noise
  of the equivalent result (for independent pixel noise)
  obtained previously (\citealt{naylor}).

Since, by assumption, we don't know the point-spread function,
  nor the noise model,
  nor the pointing or PSF jitter,
  we can't compute $\mu$ and $C$ from first principles; we have to estimate them.
Fortunately, we have $N$ epochs of data;
  we can compute a (suitably robustly estimated)
  \emph{empirical} mean and \emph{empirical} covariance
  of the original data vectors $y_n$.
It turns out this works well, at least for \project{Kepler} data.

\section{\project{Kepler} data}

\section{Comparison stars}

\section{Discussion}

Why, in fact, a forward-modeling approach ought to win in the end.
Why, nonetheless, this is not usually the chosen path these days.

How would this generalize to ``matched photometry'' across bands or images from different setups.

Intrapixel sensitivity variations;
  how do we know these happen and how does this method account for them?

What happens if you don't have good background subtraction or a DC level?

What happens if you have crowding or blending of stars?

\acknowledgments
It is a pleasure to thank
  Christian Knigge (Southampton) and
  Molei Tao (NYU)
for valuable comments.
This work was partially supported by the Moore--Sloan Data Science Environment at NYU,
  by NASA (grant NNX08AJ48G).
  and by the NSF (grant AST-0908357).
This research made use of the \project{NASA Astrophysics Data System}.

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright
\bibitem[Hogg \etal(2013)]{hoggwhitepaper}
Hogg, D.~W., Angus, R., Barclay, T., et al.\ 2013,
Maximizing Kepler science return per telemetered pixel: Detailed models of the focal plane in the two-wheel era,
\arxiv{1309.0653}
\bibitem[Naylor(1998)]{naylor}
Naylor, T.\ 1998,
An optimal extraction algorithm for imaging photometry,
\mnras, 296, 339
\end{thebibliography}

\end{document}
