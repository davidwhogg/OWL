\documentclass[12pt, letterpaper, preprint]{aastex}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\newcommand{\documentname}{\textsl{Article}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\transpose}[1]{{#1}^{\!\mathsf T}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\E}[1]{\mathrm{E}[{#1}]}
\newcommand{\Var}[1]{\mathrm{Var}[{#1}]}
\newcommand{\Covar}[1]{\mathrm{Covar}[{#1}]}
\include{vc}

\begin{document}

\title{
  Optimized ``soft'' aperture photometry for time-domain astronomy
}
\author{
  David W. Hogg,
  Dan Foreman-Mackey,
  Jonathan Goodman,
  Fengji Hou,
  others
}

\begin{abstract}
Precise stellar photometry campaigns by \project{Kepler}, \project{Spitzer}, and \project{PanSTARRS}
  (among many others)
  create precise lightcurves by fixing the star in detector coordinates,
  and estimating brightness with a fixed linear combination of background-subtracted pixel values.
Under any complete set of assumptions about the noise in the detector,
  there is a particular choice of pixel weights for use in the brightness estimate
  that maximizes signal-to-noise.
When precision requirements are extremely high,
  the noise model must necessarily include pointing and point-spread-function variations,
  which introduce highly correlated noise at the pixel level.
Here we generalize previous results on optimized photometric estimators
  to include arbitrarily correlated pixels.
Amusingly, the approach we advocate does not require any advance knowledge of any noise source;
  once there are enough epochs observed,
  the pixel-level data are sufficient to build an empirical data-driven correlated noise model.
The optimized pixel weights can be found analytically;
  when they are used to perform photometry, they work extremely well.
We demonstrate enormous improvements to the \project{Kepler} photometry of isolated stars.
The Optimized-Weight Linear (OWL) Photometry produces photometric measurements so uniform that
  they outperform or obviate much more heuristic (and much more scientifically damaging)
  ``detrending'' schemes in many real cases.
All the OWL code is available under an open-source license.
\end{abstract}

\keywords{
  instrumentation: photometers
  ---
  methods: data analysis
  ---
  methods: statistical
  ---
  techniques: photometric
}

\section{Rules of the photometry game}

Precise photometry is the bread-and-butter of the exoplanet community.
The unprecedented precision of the \project{Kepler} photometry has led to the discovery of
dozens of Earth-radius planets (HOGG CITES).
\project{Kepler}-level (part in $10^5$) precise photometry is, no doubt,
  going to be of great importance in many other areas as well.

Imagine you are trying to do very sensitive relative photometry on a variable source.
The most precise methodologies at present involve fixing the position of the source on the detector
  (cite examples)
  to reduce dependency of the results on the flat-field or flat-fielding errors.
They also often involve defocusing the telescope
  (cite examples),
  to illuminate the pixels more uniformly
  (and hence reduce intra-pixel sensitivity issues),
  avoid saturation,
  and make the observations less atmosphere-dependent.
There are also strategies related to calibration or the choices of comparison stars
  (cite examples).
Once all these choices are made,
  the choice of methodology for actually photometering the source remains.
Most projects choose something akin to aperture photometry
  (cite examples),
  in which the pixels within an aperture are co-added with unit weights,
  and pixels outside the aperture are not used at all.
Here we ask only about this last choice:
What is the best way to measure the brightness of a nearly constant source,
  given a set of precise images of the source taken over time?

We are going to answer this question using optimization, of course;
  we are going to find the ``best'' method.
We will assume that the investigator obeyed the usual rules,
  so he or she fixed the position of the star on the detector,
  defocused the telescope,
  and avoided saturation.
However, we are also going to assume---%
  as is \emph{always the case}---%
  that there are some residual,
  unaccounted-for positional offsets (jitter) and point-spread-function changes.
These offsets and changes could be random or systematic over time,
  but we will assume that they are unknown \foreign{a priori}.
That is, the telescope house-keeping data are not good enough to track them at the level we care about.
We are trying to measure \emph{extremely} precise photometry,
  at the level of $10^{-4}$ or $10^{-5}$ or better.

The conditions we have assumed might seem strange,
  but they are generic for the most precise photometry systems currently operating.
\project{Kepler}, for instance, has all of these problems,
  as does [insert projects here, like Wright's].

% in what follows the object of photometry is a ``star'' not a ``source''.

It is possible to think of this problem as the problem of finding the best strategy in a single-player game.
\emph{The Rules} of this game are as follows:
\begin{itemize}
\item
  There is copious multi-epoch single-band imaging,
  with many epochs (many images),
  and a known position in that imaging of a star of great interest (the ``object'' star).
  There is also perhaps some ``pixel mask'' indicating what pixels are permitted for use
  in the vicinity of that star.
\item
  There may or may not be a list of ``comparison'' stars
  that are used to calibrate or ratio the photometry of the object star.
  If there aren't, then it can be assumed that the imaging is very well calibrated.
\item
  It is expected that the star is fairly stationary in device coordinates (pixel location).
  However, in each image it has been offset slightly by some amount from its fiducial position.
  There are no reliable meta-data to help you understand those small offsets.
  Any comparison stars are also subject to small offsets;
  the comparison-star offsets are not necessarily the same as the object-star offsets,
  because there is camera rotation, variable optical distortion, proper motion, and parallax.
\item
  It is expected that the point-spread function (PSF) is fairly constant.
  However, again, in each image it has a small difference from the fiducial point-spread function.
  There are no reliable meta-data to help with this,
  and the comparison star PSFs will also vary but not in precisely the same way,
  though their variations might be non-trivially correlated.
\item
  There are no reliable meta-data about read noise or gains,
  so there is no precise noise estimate for any pixel at any epoch.
\item
  When you photometer a star,
  the only operation you are permitted is linear weighted sums of pixels in the images.
  Furthermore, any weighted sums must be precisely identical from epoch to epoch.
  That is, the ``aperture''
  (which in fact may be some complicated set of pixel weights)
  is required to be identical from image to image.
\end{itemize}
These rules are generic;
  this is a common situation for precise, multi-epoch photometry.

The only rule to which we object is the last one:
In principle it should be better to do focal-plane modeling
  and perform photometry as a maximum-likelihood value
  or some statistics of a posterior PDF
  based on a parameterized likelihood function (for example, \citealt{hoggwhitepaper}).
However, there are many applications in which
  the physical information one would like for building such a model is not available.
We would parameterize and fit nonetheless,
  but most astronomical projects don't take this approach at present.
There are also situations in which the output of full likelihood fitting for the data
  shows correlations between the final photometry and nuisance parameters (such as PSF parameters).
In these cases, the investigator is left wondering if the residual photometric variance
  has contributions from nuisance-parameter variance or fit residuals or model mismatch.

There have been some previous attempts to optimize photometric estimators subject to \emph{The Rules},
  in generic (but limiting) situations.
When the pointing and point-spread function are both known precisely,
  the pixel noise variance is known for every pixel,
  and the pixel noise is independent (uncorrelated) pixel-to-pixel,
  there are simple analytic results (\cite{naylor}).
These results reduce to the psf-as-matched-filter standard practice
  in the further simplification of sky-dominated noise.
There are no previous results (to our knowledge) that take account of
  the highly covariant noise created by variable pointing or optics.
This is the first primary novelty of this work.
The second primary novelty is the realization that the noise---%
  even the covariant part created by optics and pointing---%
  can be determined empirically from the data themselves,
  with no prior knowledge of the characteristics of the detector,
  telescope, or atmosphere.

\section{Well-calibrated data}

In the (possibly fantastical) situation in which the imaging data are all properly calibrated,
  we have no need for comparison stars.
We are going do the photometry using nothing but the pixels near the object star.

Before we say what ``properly calibrated'' means,
  it is necessary to specify what we mean by an ``image''.
Ordinarily it is our view that
  an image is a measurement of the smoothed intensity field
  or smoothed photon phase-space density.
It ought to have units of energy per time per area per solid angle
  or photons per time per area per solid angle.
The problem with this view of an image
  is that if we do aperture photometry---%
  that is, if we make linear weighted sums of image pixels---%
  to get a total stellar flux,
  the weights in the weighted sum must have units of \emph{solid angle}.
Small changes to the optics or atmosphere will change the astrometric mapping of the camera,
  changing the solid angles of the pixels.
Aperture photometry is the wrong thing to do in this formulation of an image;
  or it is an okay thing to do if the weights are permitted to vary with the astrometric distortion.
That would violate \emph{The Rules} above.

So the image we want for these purposes is not a measurement of the intensity field,
  but rather a kind of ``flux map''.
It ought to have units of energy per time per area
  or photons per time per area.
These values can be co-added with dimensionless weights to deliver a stellar flux.
The only difference between an intensity image and a flux map is a factor of
  the angular size of the pixels.
In many imagers pixels are similar in size so these differences aren't apparent.
However, at the levels of precision we care about here
  ($10^{-4}$ or $10^{-5}$ or better)
  these details matter.

What is meant by ``properly calibrated''?
The image (flux map) is properly calibrated when it is can be treated as a measurement of---%
  or is directly proportional to---%
  a smoothed map of energy or photon number per time per area.
It is ``smoothed'' because it is convolved (or correlated) with some finite
  and possibly spatially varying PSF
  (which we don't know).

NEED TO SAY SOMETHING ABOUT ZERO-MEAN RESIDUAL BACKGROUND SUBTRACTION.

Are any imaging data properly calibrated according to these criteria?
We don't know, but the \project{Kepler} Satellite may be.
The standard data products of the \project{Hubble Space Telescope}
  are given in intensity units,
  but the astrometric solution is known to high precision,
  so (with a tiny bit of work)
  these also can be properly calibrated for aperture photometry according to this definition.

We do not endorse the view that images should be presented as flux maps!
We are just sayin' that \emph{if} you need to obey \emph{The Rules},
  then you need to convert your intensity maps into flux maps before playing the game.

\section{Optimized photometry}

We have at each of $N$ epochs $n$ a set of $D$ pixels in a small patch
  that contains a star.
The set of measured, properly calibrated (see above),
  correctly background-subtracted (see above)
  pixel values at epoch $n$ form a $D$-dimensional column vector $y_n$.
According to \emph{The Rules}, all we are permitted to do at epoch $n$
  is produce a weighted sum
\begin{eqnarray}
q_n &\leftarrow& \transpose{w}\, y_n
\quad ,
\end{eqnarray}
  where $q_n$ is a scalar photometric measurement,
  $w$ is a $D$-dimensional column vector of weights,
  and (because of the transpose)
  the product is effectively an inner (scalar or dot) product.
\emph{The Rules} state that the weight vector $w$ must not depend on $n$ or anything else;
  the weights are the same at every epoch.

We can see $q_n$ as an estimator---a point estimate---of the stellar brightness.
If we knew the expectation value (mean) for the pixel data $y_n$ and also the
  variance of those data around the mean,
  we could predict the expectation and variance of the estimator.
The expectation value would be:
\begin{eqnarray}
\E{q_n} &=& \transpose{w}\, \mu_n
\\
\mu_n &\equiv& \E{y_n}
\\
\Var{q_n} &=& \transpose{w}\, C_n\, w
\\
C_n &\equiv& \Covar{y_n}
\quad ,
\end{eqnarray}
  where $\E{x}$ is the expectation value of $x$,
  $\Var{x}$ is the variance of (a scalar) $x$ around its expectation value,
  and $\Covar{x}$ is the $D\times D$ symmetric, positive definite covariance matrix
  of (a vector) $x$ around its expectation value.
In the simple case of independent noise from pixel-to-pixel,
  $\Covar{y_n}$ becomes diagonal,
  and the variance $\Var{q_n}$ becomes simply the sum of the diagonal variances,
  weighted by the squared weights.
The expected squared signal-to-noise ratio $\Sigma^2$ for $q_n$ is
\begin{eqnarray}
\Sigma^2 &=& \frac{[\transpose{w}\, \mu_n]^2}{\transpose{w}\, C_n\, w}
\quad .
\end{eqnarray}

It is almost always assumed that the pixel-to-pixel noise is independent (diagonal);
  why do we not assume this here?
The answer lies in \emph{The Rules}:
Variations of the pointing and point-spread function (and background),
  which are all unknown,
  correlate the pixels.
That is, they produce a kind of noise that is neither photon nor read noise in the detector.
They produce a jitter noise that leads to covariant variations of the individual pixel values.

We win the game if we find the weight vector $w$ that optimizes signal-to-noise.
It can't optimize the signal-to-noise at epoch $n$;
  it must optimize the mean or typical signal-to-noise across epochs.
That is, we want to find the weight vector $w$ that optimizes
\begin{eqnarray}
  \Sigma^2 &=& \frac{[\transpose{w}\, \mu]^2}{\transpose{w}\, C\, w}
  \quad ,
\end{eqnarray}
where now $\mu$ is the expectation value of the data vectors $y_n$ but now over \emph{all} epochs,
  and $C$ is the covariance matrix for the data but again over \emph{all} epochs.
The signal-to-noise-optimized weight vector is simply
\begin{eqnarray}
  w &\leftarrow& \inverse{C}\, \mu
  \quad ,
\end{eqnarray}
  where the inverse operation is the full matrix inverse.
This optimized-weight result is the natural generalization to correlated noise
  of the equivalent result (for independent pixel noise)
  obtained previously (\citealt{naylor}).

Previous work has called these weights ``optimal'' but they are only really ``optimized''.
For one, the word ``optimal'' always begs the question ``with respect to what?''
For another, we reserve the word ``optimal'' for estimators
  that have some glimmer of hope of saturating the Cram\'er--Rao bound (CITE).
The optimized estimators presented here will not do this in general
  (and nor will those of \citealt{naylor}),
  because they are not based on any reasonable likelihood function.

By assumption, we don't know the point-spread function,
  nor the noise model,
  nor the pointing or PSF jitter.
Therefore, we can't compute $\mu$ and $C$ from first principles; we have to estimate them.
Fortunately, we have $N$ epochs of data;
  we can compute a (suitably robustly estimated)
  \emph{empirical} mean $\hat{\mu}$ and \emph{empirical} covariance $\hat{C}$
  of the original data vectors $y_n$.
That is, we will do something like
\begin{eqnarray}
  \hat{\mu} &\leftarrow& \frac{1}{N}\,\sum_{n=1}^N y_n
  \\
  \hat{C} &\leftarrow& \frac{1}{N-1}\,\sum_{n=1}^N [y_n - \hat{\mu}]\,\transpose{[y_n - \hat{\mu}]}
  \\
  \hat{w} &\leftarrow& \inverse{\hat{C}}\, \hat{\mu}
  \quad ,
\end{eqnarray}
  where the operations are averages over the epochs,
  the vector product is the outer or tensor product,
  and technically we require $N \gg D$ (far more epochs than pixels).
It turns out that with small modifications
  (to make the means robust to outliers)
  this works well, at least for \project{Kepler} data.
The Optimized-Weight Linear (OWL) photometry $q_n$ is then
\begin{eqnarray}
  q_n &\leftarrow& \transpose{\hat{w}}\, y_n
  \quad ,
\end{eqnarray}
  that is, one scalar $q_n$ per epoch $n$, obtained by a weighted sum of the $y_n$.
We have not just obeyed \emph{The Rules},
  we have come up with the best possible strategy in terms of empirical signal-to-noise.

There is one overall scale that is free;
  any scalar multiple of the weight vector $\hat{w}$ is identical in signal-to-noise
  to any other scalar multiple.
In practice it doesn't matter how we choose this unless either machine precision is an issue,
  or else some kind of absolute aspects of photometry are required.

\section{\project{Kepler} data}

\figurename~\ref{fig:pixels} shows time histories for 19 pixels in\ldots HOGG

We compute robust statistics thusly\ldots HOGG

\figurename~\ref{fig:images} shows the empirical mean image $\hat{\mu}$,
  the diagonal elements of the empirical covariance matrix $\hat{C}$,
  the top two eigenvectors of the covariance matrix,
  and the optimized weights\ldots HOGG

\figurename~\ref{fig:results} shows the \project{Kepler} SAP flux,
  and some standard heurisitic detrendings thereof.
It also shows the the OWL photometry \ldots HOGG

\section{Comparison stars}

\section{Discussion}

Why, in fact, a forward-modeling approach ought to win in the end.
Why, nonetheless, this is not usually the chosen path these days.

How would this generalize to ``matched photometry'' across bands or images from different setups.

Intrapixel sensitivity variations;
  how do we know these happen and how does this method account for them?

What happens if you don't have good background subtraction or a DC level?

On a related note, what if you have a \emph{huge} image patch centered on the star;
  issues?  YES.

The biggest (in our view) limitation of this method is that it cannot be used
  to accurately measure the properties of stars
  that are close enough that their point-spread functions overlap,
  or stars in crowded fields.
The issue is that the optimization procedure has no knowledge or understanding
  of the PSF or even of the fact that it is a star being observed.
In our view, the only justifiable solution to this problem is forward modeling,
  in which there are parameterized beliefs about the PSF and the scene,
  and in which the output photometry comes from analysis of a likelihood function.
The only conceivable generalization of optimized linear photometry that might be relevant
  would involve taking the optimum subject to the constraint that the weights must be
  orthogonal to some estimate of the overlapping star point-spread functions.
That is an interesting idea,
  but probably no easier or more reliable than full forward modeling.
Although we consider this non-isolated problem to be the most severe limitation of what we are doing,
  we note that the tremendous results of the \project{Kepler} mission
  have almost all been generated with straight-up aperture photometry.
There certainly is a great deal that can be done in the isolated-star limit.

All of the code used in this project is available
  from \url{http://github.com/davidwhogg/OWL/} (or forks thereof)
  under the MIT open-source software license.
This code (plus some dependencies) can be run
  to re-generate all of the figures and results in this \documentname;
  this version of the paper was generated with git hash
  \texttt{\githash} (\gitdate).

\acknowledgments
It is a pleasure to thank
  Christian Knigge (Southampton) and
  Molei Tao (NYU)
for valuable comments.
This work was partially supported by the Moore--Sloan Data Science Environment at NYU,
  by NASA (grant NNX08AJ48G).
  and by the NSF (grant AST-0908357).
This research made use of the \project{NASA Astrophysics Data System}.

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright
\bibitem[Hogg \etal(2013)]{hoggwhitepaper}
Hogg, D.~W., Angus, R., Barclay, T., et al.\ 2013,
Maximizing Kepler science return per telemetered pixel: Detailed models of the focal plane in the two-wheel era,
\arxiv{1309.0653}
\bibitem[Naylor(1998)]{naylor}
Naylor, T.\ 1998,
An optimal extraction algorithm for imaging photometry,
\mnras, 296, 339
\end{thebibliography}

\clearpage

\begin{figure}
~
\caption{
The ``lightcurves'' (flux \foreign{vs} time) for 19 pixels near...HOGG
\label{fig:pixels}}
\end{figure}

\begin{figure}
~
\caption{
The empirical mean image $\hat{\mu}$,
  the diagonal elements of the empirical covariance matrix $\hat{C}$,
  the top two eigenvectors of the covariance matrix,
  and the optimized weights\ldots HOGG
\label{fig:images}}
\end{figure}

\begin{figure}
~
\caption{
The \project{Kepler} SAP flux,
  some standard heurisitic detrendings thereof,
  and the optimized-weight linear (OWL) photometry\ldots HOGG
\label{fig:results}}
\end{figure}

\end{document}
